# Docker Environment Configuration Example for ProScrape
# Copy this file to .env.docker and customize the values for your environment

# =============================================================================
# Database Configuration
# =============================================================================

# MongoDB - Main data storage
MONGODB_URL=mongodb://proscrape_user:proscrape_password@mongodb:27017/proscrape
MONGODB_DATABASE=proscrape

# PostgreSQL - Metadata and monitoring (optional)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=proscrape_db
POSTGRES_USER=proscrape_user
POSTGRES_PASSWORD=proscrape_password
POSTGRES_URL=postgresql://proscrape_user:proscrape_password@postgres:5432/proscrape_db

# Redis - Task queue and caching
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# =============================================================================
# API Configuration
# =============================================================================

# FastAPI Server Settings
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true
API_WORKERS=1

# Frontend CORS Configuration
FRONTEND_URL=http://localhost:3000
CORS_ORIGINS=["http://localhost:3000","http://localhost:8080","http://127.0.0.1:3000"]
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=["GET","POST","PUT","DELETE","OPTIONS"]
CORS_ALLOW_HEADERS=["*"]

# =============================================================================
# Scraping Configuration
# =============================================================================

# Scrapy Settings
DOWNLOAD_DELAY=1                    # Delay between requests (seconds)
RANDOMIZE_DOWNLOAD_DELAY=0.5        # Random factor for delay
CONCURRENT_REQUESTS=8               # Max concurrent requests
CONCURRENT_REQUESTS_PER_DOMAIN=1    # Max requests per domain
AUTOTHROTTLE_ENABLED=true           # Enable automatic throttling
AUTOTHROTTLE_START_DELAY=1
AUTOTHROTTLE_MAX_DELAY=60
AUTOTHROTTLE_TARGET_CONCURRENCY=2.0

# User Agent Rotation (add more for better rotation)
USER_AGENT_LIST=[
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

# Proxy Configuration (add your proxy list here)
PROXY_ENABLED=false
PROXY_LIST=[]
# Example proxy format:
# PROXY_LIST=["http://proxy1:8080","http://user:pass@proxy2:8080","socks5://proxy3:1080"]
PROXY_HEALTH_CHECK_INTERVAL=300
PROXY_MAX_CONSECUTIVE_FAILURES=3

# Spider Control
SS_SPIDER_ENABLED=true
CITY24_SPIDER_ENABLED=true
PP_SPIDER_ENABLED=true

# =============================================================================
# Celery Task Queue Configuration
# =============================================================================

# Serialization
CELERY_TASK_SERIALIZER=json
CELERY_RESULT_SERIALIZER=json
CELERY_ACCEPT_CONTENT=["json"]
CELERY_TIMEZONE=UTC
CELERY_ENABLE_UTC=true

# Scheduling (in seconds)
SCRAPING_SCHEDULE_INTERVAL=3600  # How often to run scraping (1 hour)
HEALTH_CHECK_INTERVAL=300        # Health check frequency (5 minutes)
CLEANUP_INTERVAL=86400          # Data cleanup frequency (24 hours)

# Task Routing
CELERY_ROUTES={
    "tasks.scraping_tasks.run_spider": {"queue": "scraping"},
    "tasks.scraping_tasks.health_check": {"queue": "monitoring"},
    "tasks.scraping_tasks.cleanup_old_data": {"queue": "maintenance"}
}

# =============================================================================
# Logging Configuration
# =============================================================================

LOG_LEVEL=INFO                  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT=json                 # json or text
LOG_FILE=/app/logs/proscrape.log
LOG_MAX_SIZE=10MB
LOG_BACKUP_COUNT=5

# Logging Options
ENABLE_STRUCTURED_LOGGING=true
LOG_TO_CONSOLE=true
LOG_TO_FILE=true

# =============================================================================
# Monitoring & Alerting
# =============================================================================

# Health Monitoring
HEALTH_CHECK_ENABLED=true
HEALTH_CHECK_ENDPOINTS=["/health", "/metrics"]

# Alert System
ALERTING_ENABLED=true
ALERT_EMAIL_ENABLED=false       # Set to true and configure SMTP below
ALERT_WEBHOOK_ENABLED=false     # Set to true and configure webhook below

# Email Alerts (configure if ALERT_EMAIL_ENABLED=true)
ALERT_EMAIL_SMTP_HOST=smtp.gmail.com
ALERT_EMAIL_SMTP_PORT=587
ALERT_EMAIL_USERNAME=your-email@gmail.com
ALERT_EMAIL_PASSWORD=your-app-password
ALERT_EMAIL_FROM=alerts@proscrape.com
ALERT_EMAIL_TO=admin@proscrape.com

# Webhook Alerts (configure if ALERT_WEBHOOK_ENABLED=true)
ALERT_WEBHOOK_URL=https://hooks.slack.com/your-webhook-url

# Metrics
METRICS_ENABLED=true
METRICS_INTERVAL=60
PROMETHEUS_ENABLED=false
PROMETHEUS_PORT=9090

# =============================================================================
# Security Configuration
# =============================================================================

# API Security (optional)
API_KEY_ENABLED=false
API_KEY=your-secret-api-key-here
JWT_SECRET_KEY=your-jwt-secret-key-here-make-it-long-and-random
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=30

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_BURST=10

# =============================================================================
# Data Processing Configuration
# =============================================================================

# Data Validation
STRICT_VALIDATION=false         # Set to true for strict data validation
PRICE_MIN=1000                 # Minimum reasonable price (EUR)
PRICE_MAX=10000000             # Maximum reasonable price (EUR)
AREA_MIN=10                    # Minimum area (sqm)
AREA_MAX=1000                  # Maximum area (sqm)

# Data Management
AUTO_REMOVE_DUPLICATES=true
DUPLICATE_THRESHOLD=0.95       # Similarity threshold for duplicate detection
DATA_RETENTION_DAYS=365        # How long to keep old data

# Image Processing (optional)
DOWNLOAD_IMAGES=false          # Set to true to download listing images
IMAGE_STORE_S3_BUCKET=proscrape-images
IMAGE_QUALITY=80
IMAGE_MAX_SIZE=1920x1080

# =============================================================================
# Development Settings
# =============================================================================

# Debug and Development
DEBUG=true                     # Set to false in production
DEVELOPMENT_MODE=true          # Set to false in production
TESTING_MODE=false

# Performance Monitoring
ENABLE_PROFILING=false
PROFILE_OUTPUT_DIR=/app/profiles

# Caching
CACHE_ENABLED=true
CACHE_TTL=3600                # Cache time-to-live (seconds)
CACHE_MAX_SIZE=1000           # Maximum cache entries

# Development Tools
HOT_RELOAD=true               # Auto-reload on code changes
AUTO_RESTART=true             # Auto-restart services
ENABLE_DEBUG_TOOLBAR=true     # Enable FastAPI debug toolbar

# =============================================================================
# Notes and Instructions
# =============================================================================

# 1. Copy this file to .env.docker and customize values
# 2. Never commit .env.docker to version control
# 3. For production, set DEBUG=false and DEVELOPMENT_MODE=false
# 4. Configure real proxy list for better scraping success
# 5. Set up monitoring alerts for production deployment
# 6. Use strong passwords and API keys in production
# 7. Enable SSL/HTTPS for production API endpoints