# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

ProScrape is a web scraping pipeline for Latvian real estate websites. The project is designed to scrape three specific sites:
- ss.com/en/real-estate/ (static HTML, requires standard HTTP scraping)
- city24.lv/en/ (JavaScript-heavy, requires dynamic scraping)
- pp.lv/lv/landing/nekustamais-ipasums (JavaScript-heavy, requires dynamic scraping)

## Technology Stack

- **Scraping**: Scrapy framework with scrapy-playwright for dynamic content
- **Static scraping**: BeautifulSoup/XPath for HTML parsing
- **Dynamic scraping**: Playwright for JavaScript-rendered pages
- **Data storage**: MongoDB with Motor (async driver)
- **API**: FastAPI with Pydantic for data validation and enhanced WebSocket support
- **WebSocket**: Enhanced connection manager with health monitoring and automatic reconnection
- **Task queue**: Celery with RabbitMQ/Redis
- **Task scheduling**: Celery Beat
- **Internationalization**: Multi-language support (EN/LV/RU) with dynamic translation
- **Containerization**: Docker with production-ready multi-stage builds

## Project Structure (Implemented)

The project has been successfully implemented with the following structure:
```
ProScrape/
â”œâ”€â”€ spiders/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_spider.py       # Base spider class with common functionality
â”‚   â”œâ”€â”€ ss_spider.py         # Static HTML scraper (implemented)
â”‚   â”œâ”€â”€ city24_spider.py     # Dynamic JS scraper (implemented)
â”‚   â”œâ”€â”€ pp_spider.py         # Dynamic JS scraper (implemented)
â”‚   â”œâ”€â”€ items.py             # Scrapy item definitions
â”‚   â”œâ”€â”€ middlewares.py       # Custom middlewares for UA/proxy rotation
â”‚   â”œâ”€â”€ pipelines.py         # Data validation and MongoDB storage
â”‚   â””â”€â”€ settings.py          # Scrapy configuration
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ listing.py           # Pydantic models (Pydantic v2 compatible)
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ celery_app.py        # Celery configuration
â”‚   â””â”€â”€ scraping_tasks.py    # Celery task definitions
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application with enhanced WebSocket support
â”‚   â”œâ”€â”€ i18n_endpoints.py    # Internationalization API endpoints
â”‚   â””â”€â”€ middleware/          # API middleware
â”‚       â”œâ”€â”€ cors.py          # CORS configuration
â”‚       â””â”€â”€ i18n.py          # Language detection middleware
â”œâ”€â”€ websocket/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ enhanced_manager.py  # Enhanced WebSocket connection manager
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py          # MongoDB connection utilities
â”‚   â”œâ”€â”€ proxies.py           # Proxy and UA rotation logic
â”‚   â”œâ”€â”€ normalization.py     # Data normalization utilities
â”‚   â”œâ”€â”€ translation_manager.py # Translation management system
â”‚   â””â”€â”€ logging_config.py    # Structured logging configuration
â”œâ”€â”€ translations/
â”‚   â”œâ”€â”€ en.json              # English translations
â”‚   â”œâ”€â”€ lv.json              # Latvian translations
â”‚   â”œâ”€â”€ ru.json              # Russian translations
â”‚   â””â”€â”€ properties_*.json    # Property-specific translations
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py          # Environment-based configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py          # Pytest configuration
â”‚   â”œâ”€â”€ test_models.py       # Model unit tests
â”‚   â””â”€â”€ test_normalization.py # Normalization tests
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ docker-compose.yml       # Docker service orchestration
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ run.py                  # Main entry point script
â”œâ”€â”€ test_connection.py      # Database connection tester
â”œâ”€â”€ test_celery_worker.py   # Celery worker connectivity tester
â”œâ”€â”€ test_celery_beat.py     # Celery Beat scheduler tester
â”œâ”€â”€ start_redis.bat         # Redis server startup script (Windows)
â”œâ”€â”€ stop_redis.bat          # Redis server stop script (Windows)
â”œâ”€â”€ docker-compose.redis.yml # Production Redis/Celery deployment
â”œâ”€â”€ .env                    # Environment configuration
â””â”€â”€ DEPLOYMENT.md           # Comprehensive deployment guide
```

## Development Commands

The project is fully implemented with enhanced WebSocket support and internationalization. Here are the key commands:

### Local Development
```bash
# Environment setup
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
python -m playwright install chromium

# Start Redis server
start_redis.bat  # Windows batch script
# OR: docker run -d --name redis-server -p 6379:6379 redis:7-alpine

# Test system connectivity
python test_connection.py       # MongoDB connection
python test_celery_worker.py    # Celery worker connectivity  
python test_celery_beat.py      # Celery Beat scheduler

# Run services (use separate terminals)
python run.py api       # FastAPI server with enhanced WebSocket on port 8000
python run.py worker    # Celery worker
python run.py beat      # Celery scheduler
python run.py flower    # Monitoring UI on port 5555

# Test enhanced WebSocket implementation
python test_websocket_stability.py     # Comprehensive WebSocket stability tests
python test_frontend_ws.py             # Frontend WebSocket client compatibility

# Run spiders directly
python -m scrapy crawl ss_spider
python -m scrapy crawl city24_spider
python -m scrapy crawl pp_spider

# Run tests
pytest tests/
```

### Docker Deployment (Recommended)
```bash
# Start all services with enhanced WebSocket support
docker-compose --env-file .env.docker up -d

# Start individual services
docker-compose --env-file .env.docker up -d redis  # Redis server
docker-compose --env-file .env.docker up -d api    # API with enhanced WebSocket
docker-compose --env-file .env.docker up -d flower # Monitoring UI

# Rebuild containers after code changes
docker-compose --env-file .env.docker up -d --build

# View container logs
docker-compose logs -f api    # API and WebSocket logs
docker-compose logs -f redis  # Redis logs

# Check container health
docker ps                     # Running containers
docker-compose ps             # Service status

# Stop all services
docker-compose down

# Production deployment with optimized WebSocket configuration
docker-compose -f docker-compose.websocket.yml --env-file .env.docker --profile production up -d
```

## Current Status

### Core Infrastructure âœ…
- âœ… **MongoDB**: Connected to Atlas cluster (connection string in .env)
- âœ… **Redis**: Configured with Docker for Celery and WebSocket background tasks
- âœ… **Docker**: Production-ready containerization with multi-stage builds

### Data Pipeline âœ…
- âœ… **Spiders**: All three spiders implemented and tested with proper selectors
- âœ… **Models**: Pydantic v2 compatible with Decimal handling and i18n support
- âœ… **Data Normalization**: Integrated into Scrapy pipelines with multi-language support
- âœ… **Celery**: Worker and Beat scheduler tested and working

### API & WebSocket âœ…
- âœ… **API**: FastAPI running with full CRUD endpoints and health monitoring
- âœ… **Enhanced WebSocket**: Stable connection manager with automatic health monitoring
- âœ… **Real-time Updates**: WebSocket broadcasting with ping/pong mechanism
- âœ… **Connection Management**: Unique connection IDs, state tracking, background cleanup

### Internationalization âœ…
- âœ… **Multi-language Support**: EN/LV/RU translations with dynamic switching
- âœ… **Translation Management**: Centralized translation system with caching
- âœ… **API i18n**: Language detection middleware and localized responses
- âœ… **Property Translations**: Specialized translations for real estate terminology

### Testing & Monitoring âœ…
- âœ… **Unit Tests**: Comprehensive test suite for models and normalization
- âœ… **WebSocket Tests**: Stability and frontend compatibility testing
- âœ… **Health Monitoring**: API health checks and WebSocket statistics
- âœ… **Container Monitoring**: Docker health checks and service dependencies

### Remaining Tasks
- âš ï¸ **Proxies**: Proxy rotation implemented but no proxies configured
- ğŸ”„ **Production Deployment**: Ready for production with Nginx proxy support

## Key Implementation Guidelines

### Site-Specific Considerations

1. **ss.com**: Use standard HTTP requests with proper User-Agent headers and rate limiting
2. **city24.lv & pp.lv**: Implement Playwright automation to handle JavaScript rendering and cookie consent popups

### Data Schema

All scrapers should extract data into a unified schema with these fields:
- listing_id (unique identifier)
- title
- price (normalized to EUR)
- area
- coordinates
- features
- posted_date
- image_urls

### Anti-Scraping Measures

- Implement proxy rotation for all spiders
- Use randomized request headers
- Add delays between requests
- Handle captchas and honeypot detection
- Use realistic User-Agent strings

### Database Considerations

- Use unique indexes on listing_id to prevent duplicates
- Implement Pydantic models for data validation before MongoDB insertion
- Use Motor for async MongoDB operations in FastAPI

## Next Steps (Priority Order)

### High Priority (COMPLETED)
1. âœ… **Test Scraping Functionality**
   - âœ… Run each spider with limited items to verify selectors
   - âœ… Check data quality and completeness
   - âœ… Monitor for blocking or rate limiting

2. âœ… **Implement Data Normalization**
   - âœ… Integrate `utils/normalization.py` into pipelines
   - âœ… Ensure consistent data format across all spiders
   - âœ… Handle currency conversion and date parsing

3. âœ… **Set up Redis**
   - âœ… Install Redis locally with Docker
   - âœ… Test Celery worker connectivity
   - âœ… Verify task scheduling works with Celery Beat

### Medium Priority (COMPLETED)
4. âœ… **Enhance API Endpoints**
   - âœ… Add filtering by date ranges (date_from, date_to, posted_from, posted_to)
   - âœ… Implement data export endpoints (CSV/JSON with /export/csv and /export/json)
   - âœ… Add websocket support for real-time updates (/ws endpoint with connection manager)

5. âœ… **Configure Proxy Rotation**
   - âœ… Add proxy list to settings (enhanced with health monitoring settings)
   - âœ… Test proxy health checking (background monitoring with statistics)
   - âœ… Implement automatic failover (intelligent proxy selection with success rates)

6. âœ… **Improve Error Handling**
   - âœ… Add retry logic for failed requests (exponential backoff with jitter)
   - âœ… Implement dead letter queue for failed items (comprehensive failure tracking)
   - âœ… Set up alerting for critical failures (email/webhook notifications)

### Low Priority
7. **Create Monitoring Dashboard**
   - Build React/Vue frontend
   - Display scraping statistics
   - Show data quality metrics

8. **Data Quality Validation**
   - Implement price reasonableness checks
   - Validate coordinate boundaries
   - Flag suspicious listings

9. **Automated Testing**
   - Create spider contracts
   - Add integration tests
   - Set up CI/CD pipeline

10. **Production Deployment**
    - Configure Docker Swarm/K8s
    - Set up SSL certificates
    - Implement log aggregation

## Testing and Validation

The system includes comprehensive testing utilities:

### Spider Testing Results
- **SS Spider**: Successfully extracts listings from static HTML
- **City24 Spider**: Fixed 404 issues, now scrapes from main page
- **PP Spider**: Resolved Playwright timeout issues with load strategy

### Integration Testing
- **Data Normalization**: All fields properly normalized across spiders
- **Celery Workers**: Connectivity and task queuing tested
- **Celery Beat**: Scheduler configuration and timing validated
- **Redis**: Docker-based setup working reliably

## Known Issues and Solutions

### Issue: Windows Unicode Encoding
**Solution**: Replace Unicode characters (âœ“, âŒ) with ASCII equivalents in console output

### Issue: City24 Spider 404 Errors
**Solution**: Updated from search endpoints to main page scraping with `/apartment` and `/house` patterns

### Issue: PP Spider Playwright Timeouts
**Solution**: Changed wait strategy from 'networkidle' to 'load' due to continuous JS activity

### Issue: Pydantic v2 Compatibility
**Solution**: 
- Use `field_validator` instead of `validator`
- Use `populate_by_name` instead of `allow_population_by_field_name`
- Convert Decimal to float before MongoDB insertion

### Issue: MongoDB Atlas SSL Certificate Warning
**Solution**: This is a known issue with PyMongo and can be safely ignored in development

## API Endpoints

### Core Endpoints
- `GET /` - Root endpoint
- `GET /health` - Health check
- `GET /listings` - List listings with pagination and filters (now includes date range filtering)
- `GET /listings/{listing_id}` - Get specific listing
- `GET /listings/search` - Search listings by text
- `GET /stats` - Get database statistics

### Export Endpoints
- `GET /export/csv` - Export listings to CSV format with filtering
- `GET /export/json` - Export listings to JSON format with filtering

### Real-time Updates
- `WebSocket /ws` - WebSocket endpoint for real-time listing updates

### Monitoring Endpoints
- `GET /proxy/stats` - Get proxy rotation statistics and health information
- `GET /monitoring/dead-letter-queue` - Get dead letter queue statistics
- `GET /monitoring/alerts` - Get recent alerts and alert summary
- `POST /monitoring/check-alerts` - Manually trigger alert checking
- `GET /monitoring/health` - Get comprehensive system health information

## Environment Variables

Key variables in `.env`:
- `MONGODB_URL` - MongoDB connection string (Atlas)
- `MONGODB_DATABASE` - Database name
- `REDIS_URL` - Redis connection (for Celery)
- `LOG_LEVEL` - Logging verbosity
- `DOWNLOAD_DELAY` - Delay between requests
- `*_ENABLED` - Enable/disable specific spiders

## Agent and Task Management Guidelines

- Always use agents for specific tasks